{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the function for calculating Cohen's kappa score \n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for processing the file created by prompting Llama3 through the console:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(filename):\n",
    "    ''' \n",
    "    Opens the files created by prompting Llama3 through Powershell, and finds all instances of brackets, [],\n",
    "    and adds these to a list.\n",
    "    Then runs through each element in the list, which is now a string of numbers, and splits by comma\n",
    "    and removes whitespace.\n",
    "    `filename`: a textfile with output from prompting Llama3\n",
    "    '''\n",
    "    with open(filename, 'r', encoding='utf-16le') as infile:\n",
    "        text = infile.read()\n",
    "    \n",
    "    bracket_list = re.findall(r'\\[(.*?)\\]', text)\n",
    "\n",
    "    split_num_values = np.zeros((len(bracket_list),11))\n",
    "    for i in range(len(bracket_list)):\n",
    "        comma_sp = bracket_list[i].split(',')       # splits by comma\n",
    "        x = np.zeros(11)\n",
    "        for j in range(len(comma_sp)):\n",
    "            x[j] = comma_sp[j].strip(' ')\n",
    "        split_num_values[i] = x\n",
    "\n",
    "    return split_num_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating instances for each file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the pickle-file previosly created in as a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting this dataframe to an array to compare with the other values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = df.iloc[0:, 2:-1].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_1 = text_processing(r'C:\\Users\\maria\\OneDrive - Universitetet i Oslo\\VÅR2024\\Textembedding\\Drexl\\llama_output\\output_drexl_1.txt')\n",
    "run_2 = text_processing(r'C:\\Users\\maria\\OneDrive - Universitetet i Oslo\\VÅR2024\\Textembedding\\Drexl\\llama_output\\output_drexl_2.txt')\n",
    "run_3 = text_processing(r'C:\\Users\\maria\\OneDrive - Universitetet i Oslo\\VÅR2024\\Textembedding\\Drexl\\llama_output\\output_drexl_3.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohen's Kappa score for Llama3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data from the Drexl-set can be categorized as more than one category we get a nested array containing the data. The function cohen_kappa_score cannot compute the score of nested arrays. The function below computes this score two different ways, first by line, compares each response to the ground truth, and then by category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_cohen(y1, y2):\n",
    "    ''' \n",
    "    Computes Cohen's Kappa Score for a nested array by row and by column(category).\n",
    "    `y1`: nested array\n",
    "    `y2`: nested array\n",
    "    '''\n",
    "    # by row\n",
    "    cohen_by_row = np.zeros(len(y1))\n",
    "    for i in range(len(y1)):\n",
    "        cohen_by_row[i] = cohen_kappa_score(y1[i],y2[i])\n",
    "\n",
    "    # by column/category\n",
    "    cohen_by_column = np.zeros(len(y1[0]))\n",
    "    for i in range(len(y1[0])):\n",
    "        y1_col = y1[:,i]\n",
    "        y2_col = y2[:,i]\n",
    "        cohen_by_column[i] = cohen_kappa_score(y1_col,y2_col)\n",
    "        \n",
    "    return cohen_by_row, cohen_by_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function to turn the output from the previous function into a dictionary containing values such as the mean, max and min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_dict(y1, y2):\n",
    "    a, b = nested_cohen(y1, y2)\n",
    "    datadict = {'Mean by line': np.mean(a), \n",
    "                'Max': np.max(a), \n",
    "                'Min': np.min(a), \n",
    "                'Standard deviation': np.std(a), \n",
    "                'First quantile': np.percentile(a, 25),\n",
    "                'Median': np.percentile(a, 50),\n",
    "                'Third quantile': np.percentile(a, 75),\n",
    "                'Cohen\\'s Kappa by category': b, \n",
    "                'Mean by category': np.mean(b)\n",
    "                }\n",
    "    \n",
    "    return datadict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run this function for each comparison we wish to make:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_list = [ground_truth, run_1, run_2, run_3]\n",
    "run_name = ['Ground truth', 'Run 1', 'Run 2', 'Run 3']\n",
    "\n",
    "dict_list = []\n",
    "for i in range(0,len(run_list)):\n",
    "    for j in range(i+1, len(run_list)):\n",
    "        d = data_to_dict(run_list[i], run_list[j])\n",
    "\n",
    "        updict = {'Compared runs': f'{run_name[i]} and {run_name[j]}'}          # description of which runs are being compared    \n",
    "        updict.update(d)                                                        # adds the dictionary as the first item\n",
    "        dict_list.append(updict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then want to write this lilst of dictionaries to a csv-file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# field names\n",
    "fields = ['Compared runs', 'Mean by line', 'Max', 'Min', 'Standard deviation', 'First quantile',\n",
    "          'Median', 'Third quantile', 'Cohen\\'s Kappa by category', 'Mean by category']\n",
    "\n",
    "filename = \"IRR_results.csv\"\n",
    "\n",
    "with open(filename, 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fields)         # creating a csv dict writer object\n",
    "    \n",
    "    writer.writeheader()                                        # writing headers (field names)\n",
    "    writer.writerows(dict_list)                                 # writing data rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohen's Kappa Score for Centroid Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embed_run' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mchr\u001b[39m(\u001b[38;5;241m954\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Ground truth and Centroid Method = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcohen_kappa_score(ground_truth,\u001b[38;5;250m \u001b[39m\u001b[43membed_run\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'embed_run' is not defined"
     ]
    }
   ],
   "source": [
    "print(f'{chr(954)}: Ground truth and Centroid Method = {cohen_kappa_score(ground_truth, embed_run)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excluded list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_t_c = ground_truth.copy()\n",
    "embed_ex = embed_converter('embed_news_excluded.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removes the \"unknown\" values, ie. the values where the greatest value in the distribution from the centroid method was less than 0.50, from both the ground truth and the list from the embeddings to get comparable data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "while i < len(embed_ex):\n",
    "    if embed_ex[i] == 'unknown':\n",
    "        embed_ex.pop(i)\n",
    "        ground_t_c.pop(i)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohen_kappa_score(ground_t_c, embed_ex)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "embed",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
