\section{Methodology}
Elaborate in detail the methods you used to collect the data. This should also include any potential uncertainty and errors. 
Any relevant equations should be explained either here or in the background section. 

Typically, scientific writing should be in the passive voice. The use of active voice has been more and more accepted in scientific writing, however, it is generally better to be in a passive voice. 
A passive voice removes the author from the work, which makes the work done feel more generalised, and should be replicable by anybody. 
Since scientific work should be factual by nature, the results should be the same regardless of the individual doing the experiment. 
Although this is not always true, especially considering bias, it is generally recommended to leave the subject out of the report. 

\subsection{Constructed Dataset}
\begin{flushleft}
The process started with the creation of a dataset centered around the question "Why did you choose the Honours program". 25 fictitious responses to this question were made, which made up the dataset. 
The dataset was then coded into one of the four categories "Career Opportunities", "Social Life", "Learning" and "Other", which was used for the responses that did not fit any of the aforementioned categories.
This coding formed what will henceforth be refered to as the ground truth.\\
Based on this dataset, ChatGPT UiO was prompted in different ways. \\
\begin{itemize}
    \item (In the beginning), ChatGPT UiO was simply asked to sort the responses into categories using the following prompt: \\[5pt]
    \say{We would like you to qualitatively code the responses to the question "Why did you choose the Honours-program?":}, followed by the numbered list of the responses.\\
    \item The prompt was then updated to the following:\\[5pt]
    \say{We would like you to qualitatively categorize the responses to the question "Why did you choose the Honours-program?" into a number of categories that you see fit. The following are the responses:}, followed by the numbered list of responses.
    \item and \say{Could you do the same as in the previous question, but only make 4 categories?}\\
\end{itemize}
Finally ChatGPT UiO was asked to sort the data into the four (predefined) categories using the following prompt: (prompt)\\
This process was repeated three times. The data was collected, sorted into a list and each category was assigned
a number for ease of computation.\\[10pt]

To measure the Inter Rater Reliability, Cohen's Kappa Score was used using the built-in functionality in the python package \textit{scikitlearn.metrics}.\\
(reference to the code used)\\[10pt]

We then used the centroid method on this dataset using the code provided from the Colab. - have to specify the parameters used

\subsection{News Headlines Dataset}
For this project a pre-categorized dataset from Hugging Face was used (\url{https://huggingface.co/datasets/okite97/news-data}), containing news headlines which had been labeled with the following categories: "politics", "business", "health", "sports", "tech" and "entertainment".
The dataset had already been split into a training- and test-dataset, of which the test-dataset was used for this project. 
The test dataset consisted of 828 headlines, of which the first 500 were used. \\

\subsubsection{ChatGPT UiO}
ChatGPT UiO was then manually prompted to categorize the first 500 headlines by giving it chunks of 50 headlines at a time and starting a new chat per chunk. This prompting 
was done using the following prompt:\\[5pt]
\say{Please qualitatively code the news headlines into the seven categories "politics", "business", "health", “sports”, “tech” and "entertainment". Please write your response into one array with 50 components, one for each headline. For example, if the possible categorizations is "red" and "blue" and headline one is categorized as "red" and headline 2 as "blue", this would be written as ["red", "blue"]. Make sure to give a list with only the categories, not the headlines themselves. The following are the 50 headlines: },
 followed by 50 headlines.\\[5pt]
The prompt had to be somewhat modified in the process, since the format of the output from GPT varied, despite the specifications in the prompt. 
For example, often the additional prompt \say{Could you rather give a list with only the categories?} had to be used to get output of the current format.
The dataset of the 500 first datapoints were coded by ChatGPT UiO three separate times. A complete log of the inputs and outputs of all the three rounds can be found in the appendix.\\[10pt]
The correctly formatted outputs were then collected into a txt file, which was processed using the following code:\\
(code)\\
%\lstinputlisting[language=Python]{filename_here}\\ 
The produced lists were then used as a basis for calculating the Inter Rater Reliability using Cohen's Kappa test using the same procedure with the constructed dataset.\\

\subsubsection{Centroid method}

In addition, the centroid method was also used to obtain a categorization. 
The centroid method gives its result as a probability distribution between the different categories. 
Two different scripts were written to obtain the category the model deemed more probable.
First the parameter $\alpha$ was set to $150$ in order to obtain a confidence value greater than 0.50 for one category for all of the headlines. 
A Cohen's Kappa Score was then obtained for this categorization compared to the ground truth using the following code:\\
(code)\\
%\lstinputlisting[language=Python]{filename_here}\\ 
the parameter $\alpha$ was set to $15$, and the data where the greatest value in the distribution was smaller than 0.50 was excluded. 
A ground truth was constructed where the same values were exlcuded, 
and a Cohen's Kappa Score was obtained using the following code:\\
(code)\\
%\lstinputlisting[language=Python]{filename_here}\\ 

\subsubsection{Llama3}
Categorization was done using Llama3 in two ways: 
\begin{enumerate}
\item Prompting in the terminal using a Powershell script.
\item Importing ollama and using ollama.chat.
\end{enumerate}

The prompting in the Powershell was done using the following code:\\
(code)\\
%\lstinputlisting[language=Python]{filename_here}\\ 

The prompting with the 3"cloud server" was done using the following code:\\
(code)\\
%\lstinputlisting[language=Python]{filename_here}\\ 

\section{Drexl Dataset}
\subsection{Llama3}
Categorization of the Drexl-dataset were done by Llama3 by prompting through the terminal using a Powershell-script that reads the responses from a file
and writes the output from Llama3 to a new file. The prompt was the following:\\
\say{"I will give you a text that is a response to why a female student chose to study physics. 
I want you to qualitatively categorize this response into however many of the categories I will provide you as you see fit. The following are the categories: Attainment Value, Cost, Intrinsic value, 
Media Triggered Intrinsic Value, Event Triggered Intrinsic Value, Intrinsic Value (Social), Utility Value, Mastery Experience,
Physiological or Emotional Arousal, Social Persuasion, Vicarious Experience. I want you to give the response as only a list with a length of 11 where a 1 indicate that a response belongs to the category, and a 0 indicates no belonging.
An example of this format is [1,0,1,0,1,0,1,0,1,0,1]. Do not explain your reasoning and give only the list as your response."}\\[5pt]

Some of the responses, response 179, 327, 376, 668, 1437, 1771, 2061, 2090, had to be removed since Llama3 was unable to categorize them. In addition to this, row 501 was removed due to being empty. 
This process was repeated three times. 

\subsection{Ground truth}
The responses that created problems for Llama3 was also removed from the ground truth to make the data comparable. Additionally, the categories "Intrinsic Value" and "Intrinsic Value (Astronomy)" were merged into one category
called "Intrinsic Value", as were "Cost (Emotional)", "Cost (Outside Effort)", "Cost (Loss)" and "Cost(Task Effort)"  into "Cost". 

\subsection{Centroid Method}
In creating the centroids, a list with the responses that were only categorized into a single category were created. From this list nine responses were chosen randomly from each category. Some categories did not have a sufficient number of responses for this.
For the categories "Attainment Value", "Cost" and "Mastery Experience" more were created by going through the list of responses and singling out excerpts/sentences that the researcher thought fit the category. For "Attainment Value" and "Cost" seven such centroids were made,
for "Mastery Experience" four were made. These were then added to the list and the centroid for each category was created as a mean for the embedding of these nine chosen responses.

\end{flushleft}