\section{Methodology}
Elaborate in detail the methods you used to collect the data. This should also include any potential uncertainty and errors. 
Any relevant equations should be explained either here or in the background section. 

Typically, scientific writing should be in the passive voice. The use of active voice has been more and more accepted in scientific writing, however, it is generally better to be in a passive voice. 
A passive voice removes the author from the work, which makes the work done feel more generalised, and should be replicable by anybody. 
Since scientific work should be factual by nature, the results should be the same regardless of the individual doing the experiment. 
Although this is not always true, especially considering bias, it is generally recommended to leave the subject out of the report. 

\subsection{Constructed Dataset}
\begin{flushleft}
The process started with the creation of a dataset centered around the question "Why did you choose the Honours program". 25 fictitious responses to this question were made, which made up the dataset. 
The dataset was then coded into one of the four categories "Career Opportunities", "Social Life", "Learning" and "Other", which was used for the responses that did not fit any of the aforementioned categories.
This coding formed what we will henceforth refer to as the ground truth.\\
Based on this dataset, ChatGPT UiO was prompted in different ways. \\
\begin{itemize}
    \item (In the beginning), ChatGPT UiO was simply asked to sort the responses into categories using the following prompt: (prompt).
    \item The prompt was then updated to the following: (prompt)\\
    \item and (prompt)\\
\end{itemize}
Finally ChatGPT UiO was asked to sort the data into the four (predefined) categories using the following prompt: (prompt)\\
This process was repeated three times. The data was collected, sorted into a list and each category was assigned
a number for ease of computation.\\[10pt]

To measure the Inter Rater Reliability, Cohen's Kappa Score was used using the built-in functionality in the python package \textit{scikitlearn.metrics}.\\
(reference to the code used)\\[10pt]

We then used the centroid method on this dataset using the code provided from the Colab. - have to specify the parameters used

\subsection{News Headlines Dataset}
For this project we used a pre-categorized dataset from Hugging Face \url{https://huggingface.co/datasets/okite97/news-data}, containing news headlines which had been labeled with the following categories: "politics", "business", "health", "sports", "tech" and "entertainment".
The dataset had already been split into a training- and test-dataset, and we used the test-dataset which consisted of 828 headlines and used the first 500. \\
We then proceeded to manually prompt ChatGPT UiO by asking it to categorize the first 500 headlines by giving it chunks of 50 at a time and starting a new chat per chunk. This prompting 
was done using the following prompt:\\
(prompt)\\
The prompt had to be somewhat modified in the process, since the format of the output from GPT varied, despite the specifications in the prompt. (Explain further)\\[10pt]
The dataset of the 500 first datapoints were coded by ChatGPT UiO three separate times and each run was collected into a single document. This text was processed using the following code:\\
(code)\\
The produced lists were then used as a basis for calculating the Inter Rater Reliability using Cohen's Kappa test using the same procedure as we did in (first project).

We also used the centroid method to obtain a categorization. The centroid method gives its result as a distribution between the different categories. We wrote a script to obtain only the most probable category two different ways.
First we set $\alpha = 150$ to ~shift the weight to get a value greater than 0.50 for all of them~ and compared this categorization to the ground truth using Cohen's Kappa Score using the following code:\\
(code)\\
We then set $\alpha = 15$ and excluded the data where the greatest value in the distribution was smaller than 0.50, and constructed a ground truth where we excluded the same values, 
and used Cohen's Kappa Score to compare the two using the following code:\\
(code)\\

Llama\\
Categorization was done using Llama3 but using prompting in the terminal using a Powershell script, and using 3"cloud server".\\
The prompting in the Powershell was done using the following code:\\
(code)
The prompting with the 3"cloud server" was done using the following code:\\
(code)

\section{Drexl Dataset}
- removed row 501 - empty

\end{flushleft}
